---
title: "Neural Language Translation"
excerpt: "<br/><img src='/files/images/nmt1.png'>"
collection: portfolio
---
[Neural Language Translation](http://yueqiusun.github.io/files/NLP_final_report.pdf)<br/>

The encoder-decoder model has been used in most of the Neural Translation Systems in recent years. With the discovery of attention mechanism, the translation performance is greatly improved. In this project, we aim to compare the translation quality between simple RNN based model, Luong attention RNN based model, and self-attention based model for Chinese to English translation (Ch2En) and Vietnamese to English translation (Vi2En), respectively. It turns out that after equipping attention into the encoder-decoder model, BLEU can be improved at least 90\% for both Ch2En and Vi2EN. Also, we demonstrate that LSTM with Luong attention sequence-to-sequence model is the best model that can achieve 12.75 BLEU on character-level Chinese and 21.30 BLEU on Vietnamese.





